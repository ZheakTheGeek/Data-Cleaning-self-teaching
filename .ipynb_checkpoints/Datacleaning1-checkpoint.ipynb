{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fourth-annual",
   "metadata": {},
   "source": [
    "# Importing CSV\n",
    "\n",
    "Often times data will be retrieved in the form of an excel document, we need to turn this into something python can read, luckily the pandas library can handle that for us! If you haven't already, make sure you use pip install pandas in your command line, as you will need it's dependancies before we get started. \n",
    "\n",
    "The code below does several things. \n",
    "First we import our library (pandas)\n",
    "\n",
    "secondly, we take each of our CSVs and assign them to a variable that is easier to work with, for this example, I will use df (datafile) to refer to my data sets. \n",
    "\n",
    "Finally, we can use the describe() method to get a table of our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "continuing-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp      trtbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg    thalachh        exng     oldpeak         slp         caa  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "            thall      output  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "df2 = pd.read_csv('o2Saturation.csv')\n",
    "\n",
    "#shows us a table of collumns and rows of our data, shows the mean, std, min, max, interquartile range\n",
    "df.describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "mobile-collect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>98.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3585.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>98.239275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.726336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>96.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>97.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>98.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>98.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              98.6\n",
       "count  3585.000000\n",
       "mean     98.239275\n",
       "std       0.726336\n",
       "min      96.500000\n",
       "25%      97.600000\n",
       "50%      98.600000\n",
       "75%      98.600000\n",
       "max      99.600000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-starter",
   "metadata": {},
   "source": [
    "# Null values\n",
    "\n",
    "Clearly missing values will impact the results for our data, so lets see how complete our data set is.\n",
    "\n",
    "First let's see how many fields we are missing.\n",
    "Since this data set is relatively complete, we don't expect to see any \"True\" results, which would indicate null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "mineral-harvard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         False\n",
       "sex         False\n",
       "cp          False\n",
       "trtbps      False\n",
       "chol        False\n",
       "fbs         False\n",
       "restecg     False\n",
       "thalachh    False\n",
       "exng        False\n",
       "oldpeak     False\n",
       "slp         False\n",
       "caa         False\n",
       "thall       False\n",
       "output      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find any null values in our data\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-daniel",
   "metadata": {},
   "source": [
    "# Percentage null data\n",
    "\n",
    "Since we know in this example that none of our values have any null data, we can move on, but what if you DID have null data?\n",
    "We would want to know what percentage of that field was missing data, we can do this by running the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "awful-scene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0.0\n",
       "sex         0.0\n",
       "cp          0.0\n",
       "trtbps      0.0\n",
       "chol        0.0\n",
       "fbs         0.0\n",
       "restecg     0.0\n",
       "thalachh    0.0\n",
       "exng        0.0\n",
       "oldpeak     0.0\n",
       "slp         0.0\n",
       "caa         0.0\n",
       "thall       0.0\n",
       "output      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-cloud",
   "metadata": {},
   "source": [
    "# Dropping duplicates \n",
    "An important part of Data Sciene and Data cleaning in general is making sure that we don't have excess numbers that skew our results. \n",
    "That being said, it should be obvious why removing duplicate data values is important to getting accurate results.\n",
    "\n",
    "### drop_duplicates(inplace =True)\n",
    "\n",
    "This code causes the data value in the current field to be dropped if it is in fact a duplicate of another value. \n",
    "For those already familiar with python, this is very similar to using array.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sacred-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-australia",
   "metadata": {},
   "source": [
    "# Incomplete Data Sets\n",
    "\n",
    "While this has no impact on the data set using in this example, what could we do if our data includes values that are empty (null)?\n",
    "\n",
    "These results might skew our observation or experiment, so it is important to conider these things before we get into working with the data itself.\n",
    "\n",
    "In the below example, we are trying to determine whether or not a certain amount of fields are blank, and if they are, we will remove the collumn from our list of considered items. \n",
    "\n",
    "In the example below, we will use a threshold of 60% by creating a new threshold variable, and applying it to a dropna() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "suitable-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = len(df)*.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-stamp",
   "metadata": {},
   "source": [
    "# Checking AXIS\n",
    "\n",
    "\n",
    "the axis here refers to whether or not we are dropping collumns or rows, in this case, we will drop collumns with values that are too low\n",
    "### row = 0, collumn = 1\n",
    "if the data has too many null values, it is simply not relevant enough to the study to inclue (it will skew the data disproportionately to reality,\n",
    "or it doesn't corelate at all)\n",
    "\n",
    "additionally, knowing the shape of our data is important \n",
    "first we will check our collumns to see if they meet our threshold cutoff, once we have done that we will work with rows\n",
    "since we have 6 rows in this example project, let's cut out any rows that have 5 or less values\n",
    "again, this is a relatively complete data set, meaning we don't have many null values\n",
    "this type of cleaning becomes EXTREMELY important in larger data sets that are missing relevant information\n",
    "since incomplete results will skew our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "vocational-logistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 14)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(thresh=thresh, axis =1).shape\n",
    "df.dropna(thresh=5,axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-principle",
   "metadata": {},
   "source": [
    "# Filling in missing values \n",
    "\n",
    "### Sometimes missing values are useful, but what if we have significant data in our entry, but are missing 1 value?\n",
    "\n",
    "if you were working with machine learning, you could choose to replace missing row values with the mean or median of the group, even IQR\n",
    "this could be useful in instances where, for example, you are conducting a survey, and partiticipants chose not to answer a certain question. \n",
    "that kind of thing COULD be meaningful for very specific types of surveys and help mitigate response bias, or various other factors, depending\n",
    "on the survey method and participants perception of anonymity. This is mostly used for quantitavive values, but with some modifiation could also\n",
    "be used for qualitative ones as well.\n",
    "\n",
    "fill any age field that is missing with the median age of participants \n",
    "this is an example of the numeric data manipulation to \"autofill\" this section for live participants who might be chatting with assistants who\n",
    "they may feel the need to skew the data for (or something, just a rough example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "checked-renewal",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.age.fillna(df.age.median()).isnull().any()\n",
    "# the same could be done for the mean, these are both very common methods for handling missing values, especially with larger sample sizes\n",
    "df.age.fillna(df.age.mean()).isnull().any()\n",
    "\n",
    "# both of these methods will return false, because again, we have no null values, but these would be effective in data sets that are incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-taxation",
   "metadata": {},
   "source": [
    "# Text Management\n",
    "next we will talk about text management in theoretical terms. For this example, all of our data is quantitative, but what if for \"sex\" we allowed\n",
    "for nonbinary identifaction (consider the impacts of HRT on heart disease for example, what if we wanted to track data like this?)\n",
    "due to the nature of this particular file, that isn't something we can look at, so let's just consider the possibility and include code to do so.\n",
    "\n",
    "#first we would need to ensure that all of the entered values were all the same case, that way we can more easily work with the data.\n",
    "#henceforth throughout the project, we will refer to this as normalization, not to be confused with vector normalization.\n",
    "\n",
    "First, let's check the first 5 values using the head() method (as a side note, if we wanted to check the end, we could use tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "specific-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-jenny",
   "metadata": {},
   "source": [
    "The collumn on the left is denoting the index at which the value is stored, while the value on the right is telling us what gender the individual is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-postcard",
   "metadata": {},
   "source": [
    "# Changing Text data to consitant types \n",
    "\n",
    "the following code snippet will allow us to change all of our code to lower case strings, which is useful to make sure all categories are consistant for later use. \n",
    "\n",
    "df.desc.apply(lambda x : x.lower())\n",
    "we could also use x.title() or x.upper()\n",
    "\n",
    "it simply depends on what is easiest for you to read, work with, and is really up to your preference!\n",
    "Since our values are integers, it won't be used in this example, but it's here for you to work with if you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-damages",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
